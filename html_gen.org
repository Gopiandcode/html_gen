* HtmlGen
A simple little static templating engine for HTML.
Coded using literate programming.


* Preamble
While the overall nature of this project is quite simple - just a bit of file loading and exports, we can leverage rust's ecosystem to make our development a little easier.

** Crates
The crates we'll be using are as follows:
- *ArgParse* - This is a crate that I used a while back when making another command line application. It provides a very nice rustic interface over a library which produces command line interfaces compliant with most unix/linux standards.
an old fashioned regex.
#+begin_src rust :tangle src/main.rs  :comments org
extern crate argparse;
#+end_src

- *Regex* - We'll be taking advantage of this regex crate to make the parsing phase a little easier; while the stdlib provides some pretty useful string matching utilities, they don't quite match up to
#+begin_src rust :tangle src/main.rs  :comments org
extern crate regex;
#+end_src

** Standard Library Imports
We'll be using the path utilities provided by the standard library to help us navigate the filesystem in a cross platform way.
#+begin_src rust :tangle src/main.rs :comments org
use std::path;
#+end_src
** Module structure
We'll be splitting up our codebase as follows:

#+begin_src rust :tangle src/main.rs :noweb yes :comments org
<<modules>>
#+end_src


* Command Line Interface
Clearly this project is going to be a command line application, as the static generator will need to parse a document and construct the components.

Using argparse - as imported in the preamble, we'll design a sweet and sexy interface to access our application. The main actions we'll allow a user to perform using this application will be as follows:
- *specify output folder* - by default the output of the compiled files are placed in ~./bin/~ dir, which is made if it does not exist.
- *specify template folder* - within a non-templated file, when a template reference is used, by default the application searches the 
 ~./template/~ dir to resolve these references.
- *specify input folder* - by default the program searches ~./src/~ for the source files to be compiled

#+begin_src rust :tangle src/main.rs :comments org :noweb yes
fn main() {
 <<high level interface>>
}
#+end_src
Now, as specified, our output and template paths take default values from where the application was instantiated.
#+name: high level interface
#+begin_src rust :comments noweb
    let mut output_path = String::from("./bin");
    let mut template_path = String::from("./template");
    let mut input_path = String::from("./src");
#+end_src


Using argparse, we can implement this cmdline interface as follows:
#+name: high level interface
#+begin_src rust :comments noweb
    {
        let mut ap = argparse::ArgumentParser::new();
        ap.set_description("Simple templating engine for html documents");
        ap.refer(&mut output_path)
        .add_option(&["-o","--output"], 
                    argparse::Store, 
                    "directory for the output files to be saved");

        ap.refer(&mut template_path)
        .add_option(&["-t","--template"], 
                    argparse::Store, 
                    "directory to be searched to find templates");

        ap.refer(&mut input_path)
        .add_option(&["-i","--input"], 
                    argparse::Store, "
                    directory in which the source files to be compiled are located");


        ap.parse_args_or_exit();
    }
#+end_src

* Core Logic
Now we've obtained the directory for the files to be stored, we can move on to the main logic of the program.
Fundamentaly the logic of this program can be split into two main components:
 - Recursively descending the source directory, keeping track of the file structure.
#+name: modules 
#+begin_src rust :comments noweb
mod crawler;
#+end_src
 - Extracting the data from a given file
#+name: modules 
#+begin_src rust :comments noweb
mod parser;
#+end_src
 - generate a compiled html file from the template and save it to a folder
#+name: modules
#+begin_src rust :comments noweb
mod generator;
#+end_src 

Thus the high level execution of the system is as follows:
#+name: high level interface
#+begin_src rust :comments noweb
println!("{:?}", parser::parse_source_string("  \n#+template: simple\nbody: \\ Kiran is the best\\ \\¬¬\ntitle: We are the \\b best <champions></champions> ¬"));
#+end_src


** Parser Logic
Before we begin, we'll need the following packages in our parser:
#+begin_src rust :tangle src/parser.rs :noweb yes :comments org
use std::collections::HashMap;
use regex::Regex;
<<structures>>
#+end_src
Once again, our core specification for the parser is to extract a set of key value pairs. Our syntax will be of the following form:
#+begin_src 
ID := (Sigma/{:, (, )})+
INTRO := #+template: Sigma+\n
MAPPING := ID:  ((SIGMA/{¬})|\¬)* ¬
DOCUMENT := INTRO MAPPING*
#+end_src
Our parser will take in a string (the contents of the file), and return either a hashmap of values and a template name, or an error.
#+begin_src rust :tangle src/parser.rs :noweb yes :comments org
<<source parsing utility functions>>

pub fn parse_source_string(source: &str) 
   -> Result<(String, HashMap<String,String>),ParseError> {
<<source parsing regexes>>
<<source parsing code>>
}

#[cfg(test)]
mod test {
   use super::*;

  <<source parsing tests>>
}
#+end_src
Where a parsing error will be one of the following:
 - **Template not found** - if the source file does not specify a template to be loaded
 - **Invalid identifier** - if an identifier contains an invalid character.
 - **Unterminated Body** - if a body does not have a valid terminator.
#+name: structures
#+begin_src rust :comments noweb
#[derive(Debug)]
pub enum ParseError {
   TemplateNotFound,
   InvalidIdentifier,
   UnterminatedBody
}
#+end_src
For simplicity, we're making the parser as general as possible and opting to make failure as unlikely as possible.

To do the parsing, first we start off by consuming the template directive, and failing if not present.

First, we check that the template contains a template directive - we're leaving resolving the template to a file to a later point.
#+name: source parsing code
#+begin_src rust :comments noweb
if !source.trim_left().starts_with("#+template:") {
   return Err(ParseError::TemplateNotFound);
}
#+end_src

This means that if a source does not start with a directive, its parsing will fail:
#+name: source parsing tests
#+begin_src rust :comments noweb
#[test]
fn must_start_with_template_directive() {
   assert!(parse_source_string("temp-justkidding\n id:\n #+template:\n").is_err());
}
#+end_src

After this check, we can safetly consume the first part of the string.
#+name: source parsing code
#+begin_src rust  :comments noweb
let source = source.trim_left().split_at(11).1;
#+end_src

Next, let's retrieve the actual template name - failing if it was not provided.
#+name: source parsing code
#+begin_src rust :comments noweb
let (raw_template_name, remaining_string) = split_at_pattern(source, "\n");
let template_name = raw_template_name.trim();
if template_name.is_empty() {
   return Err(ParseError::TemplateNotFound);
}
#+end_src

This also means that if a source does not provide a template name its parsing will fail:
#+name: source parsing tests
#+begin_src rust :comments noweb
#[test]
fn must_provide_template_name() {
    assert!(parse_source_string("#+template: example\n").is_ok());
    assert!(parse_source_string("#+template:\n").is_err());
    assert!(parse_source_string("#+template:    \n").is_err());
    assert!(parse_source_string("#+template:   \n  \n").is_err());
    assert!(parse_source_string("#+template:   \t  \n").is_err());
}
#+end_src


Now, our remaining task is to simply iterate through the remaining ~ID: DATA~ pairs, and accumulate these values into a hashmap - let's begin
by setting up an initial hashmap to store the files.
#+name: source parsing code
#+begin_src rust :comments noweb
let mut data : HashMap<String, String> = HashMap::new();
#+end_src
Next, we'll define a simple loop to do the accumulation - it will use a reference to the hashmap, and the source:
#+name: source parsing code
#+begin_src rust :comments noweb :noweb yes
let mut completed = false;
let mut source = remaining_string;
let mut data = data;

while !completed {
   <<source pairs loop>>
}
#+end_src
To extract the keys and bodies, we'll be using a regex - it checks that the start of the string consists of non terminator characters,
followed by a colon.
#+name: source parsing regexes
#+begin_src rust :comments noweb :noweb yes
let key_regex = Regex::new("^[^¬:]*:").unwrap();
#+end_src

Now, inside the loop, we'll use the regex to extract the key values - for this purpose, we'll define a custom ~split_by_regex~ function,
which operates like the ~split_at_pattern~ function but uses the first match of a regex to split the input.

#+name: source parsing utility functions
#+begin_src rust :comments noweb
fn split_at_regex<'a>(string: &'a str, pat: &Regex) -> (&'a str, &'a str) {
  if let Some(m) = pat.find(string) {
     string.split_at(m.end())
  } else {
     (&"", string)
  }
}
#+end_src
Now, using this function, we can implement the key extraction.

#+name: source pairs loop
#+begin_src rust :comments noweb
let (raw_key_name, remaining_string) = split_at_regex(source, &key_regex);
let key_name = raw_key_name.trim();
source = remaining_string;
#+end_src

Now due to the way we're extracting the values, bad input may lead to an incorrect parse - we'll try and avoid this by printing an error when the IDs are wrong:
#+name: source pairs loop
#+begin_src rust :comments noweb
if key_name.len() == 0 {
  eprintln!("Invalid parse, found empty/malformed ID tag");
  return Err(ParseError::InvalidIdentifier);
}
#+end_src
Due to the way we extract the ids, we also end up bringing the colon as well. Let's just remove it before proceeding:
#+name: source pairs loop
#+begin_src rust :comments noweb
let mut key_name = key_name.to_string();
key_name.pop();
let key_name = key_name.trim();
#+end_src

Now we can move on to extracting the data. Let's start by defining a regular expression to isolate specific syntax we wish to capture.
#+name: source parsing regexes
#+begin_src rust :comments noweb
let data_regex = Regex::new("^(\\\\¬|([^¬\\\\]|\\\\[^¬])*)*¬").unwrap();
#+end_src

The regex we're using can be explained as follows; the outermost kleene closure captures the main constraint that the data should start from the start of the string and end at the first occurrance
of a terminating character.
#+begin_src regex
^ INTERNALS *¬
#+end_src

Next, for the contents of a body, we have to capture 2 main cases:
- When the character is normal and non interesting
- When the character is an escaped terminator.
#+begin_src regex
INTERNALS ::= (ESCAPED_TERMINATOR|NORMAL_CHARACTERS)
#+end_src

For the escaped terminator case, we simply match on a backspace followed by a terminator.
#+begin_src regex
ESCAPED_TERMINATOR = \¬
#+end_src

In the case of normal characters, either 
- the character is neither a backslash or a terminator
- the character is a backslash and is followed by anything other than a terminator
#+begin_src regex
NORMAL_CHARACTERS = ([^¬\\\\]|\\\\[^¬])*
#+end_src

Using this regex we can trivially extract the data, repeating the code for key extraction.
#+name: source pairs loop
#+begin_src rust :comments noweb
let (raw_data, remaining_string) = split_at_regex(source, &data_regex);
let src_data = raw_data.trim();
source = remaining_string;
#+end_src

While it is fine for data to be empty, we always require the user to provide the end character, so the string should never be 0.
#+name: source pairs loop
#+begin_src rust :comments noweb
if src_data.len() == 0 {
  eprintln!("Invalid parse, found body with no terminating tag.");
  return Err(ParseError::UnterminatedBody);
}
#+end_src

Now, as before, let's remove the terminating character.
#+name: source pairs loop
#+begin_src rust :comments noweb
let mut src_data = src_data.to_string();
src_data.pop();
let src_data = src_data.trim();
#+end_src

Finally, now we've extracted the id and the tag, we can simply put the values into our hashmap.
#+name: source pairs loop
#+begin_src rust :comments noweb
data.insert(key_name.to_string(), src_data.to_string());
#+end_src

Now, we also need to check for a terminating condition - we'll do this by checking if the remaining string, when trimmed, is empty.
#+name: source pairs loop
#+begin_src rust :comments noweb
if source.trim().is_empty() {
    break;
}
#+end_src

Finally, now that string has been consumed, we can simply return the template name and the populated hashmap.

#+name: source parsing code
#+begin_src rust :comments noweb :noweb yes
Ok((template_name.to_string(), data))
#+end_src

Aside: Notice, that during the parsing, we're using our own custom function to allow us to split by a pattern, a feature the
stdlib doesn't seem to provide.

This utility function splits a string by the first occurance of a pattern returning a string up to the first occurrance 
of the pattern and a string continuing from the pattern - the second string contains the text matching the pattern.
#+name: source parsing utility functions
#+begin_src rust :comments noweb
fn split_at_pattern<'a>(string: &'a str, pat: &str) -> (&'a str, &'a str) {
  if let Some(ind) = string.find(pat) {
     string.split_at(ind)
  } else {
     (&"", string)
  }
}
#+end_src

** Crawler Logic
The core logic for the crawler is to descend the input directory, keeping track of the current path, pass each file through the parser, then pass on the generated mapping to the generator, along with a corresponding template file and output file.

We'll be importing the following libraries for doing the core logic.
#+name: crawler imports
#+begin_src rust :comments org
use std::fs;
use std::path::Path;
#+end_src
We'll also be bringing in the parsing function from the parser, and the generator function from the generator.
#+name: crawler imports
#+begin_src rust :comments org
use parser::{parse_source_string,ParseError};
use generator::generate_output;
#+end_src

The main structure for the crawler is as follows.
#+begin_src rust :tangle src/crawler.rs :noweb yes :comments org
<<crawler imports>>

<<crawler structures>>

<<crawler function>>
#+end_src

Our crawling function, takes as input the input directory, the output directory, the template directory.
#+name: crawler function
#+begin_src rust :noweb yes :comments org
pub fn crawl_directories(output_string :&str, input_string: &str, template_path: &str) -> Result<u32,CrawlError> {
<<crawler main logic>>
}
#+end_src

The errors produced by the crawler are as follows.
- *ParseError* - When a parser occurs
- *GeneratorError* - when a generator occurs
#+name: crawler structures
#+begin_src rust :noweb yes :comments org
pub enum CrawlError {
  ParseError(ParseError),
  GeneratorError
}
#+end_src


Now, to code the main logic of the function.

Oh wait, I'm bored, we'll do it later.
#+name: crawler main logic
#+begin_src rust :noweb yes :comments org
Err(CrawlError::GeneratorError)
#+end_src

** Generator Logic

#+begin_src rust :tangle src/generator.rs :noweb yes :comments org
#+end_src

